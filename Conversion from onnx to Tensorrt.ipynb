{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hEZWcDPNGHoy"
   },
   "source": [
    "# ***Human Activity detection using 3DCNN***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZZIMbDGGg5s"
   },
   "source": [
    "## 1. Download the UFC50 Dataset and visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvbILvVZGiZr"
   },
   "source": [
    "## 2. Install Dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8201] # /usr/src/tensorrt/bin/trtexec --onnx=/nvdli-nano/data/Inference/conv3D_model_best.onnx --saveEngine=/nvdli-nano/data/Inference/conv3D_model_best.trt\n",
      "[12/17/2023-07:57:07] [I] === Model Options ===\n",
      "[12/17/2023-07:57:07] [I] Format: ONNX\n",
      "[12/17/2023-07:57:07] [I] Model: /nvdli-nano/data/Inference/conv3D_model_best.onnx\n",
      "[12/17/2023-07:57:07] [I] Output:\n",
      "[12/17/2023-07:57:07] [I] === Build Options ===\n",
      "[12/17/2023-07:57:07] [I] Max batch: explicit batch\n",
      "[12/17/2023-07:57:07] [I] Workspace: 16 MiB\n",
      "[12/17/2023-07:57:07] [I] minTiming: 1\n",
      "[12/17/2023-07:57:07] [I] avgTiming: 8\n",
      "[12/17/2023-07:57:07] [I] Precision: FP32\n",
      "[12/17/2023-07:57:07] [I] Calibration: \n",
      "[12/17/2023-07:57:07] [I] Refit: Disabled\n",
      "[12/17/2023-07:57:07] [I] Sparsity: Disabled\n",
      "[12/17/2023-07:57:07] [I] Safe mode: Disabled\n",
      "[12/17/2023-07:57:07] [I] DirectIO mode: Disabled\n",
      "[12/17/2023-07:57:07] [I] Restricted mode: Disabled\n",
      "[12/17/2023-07:57:07] [I] Save engine: /nvdli-nano/data/Inference/conv3D_model_best.trt\n",
      "[12/17/2023-07:57:07] [I] Load engine: \n",
      "[12/17/2023-07:57:07] [I] Profiling verbosity: 0\n",
      "[12/17/2023-07:57:07] [I] Tactic sources: Using default tactic sources\n",
      "[12/17/2023-07:57:07] [I] timingCacheMode: local\n",
      "[12/17/2023-07:57:07] [I] timingCacheFile: \n",
      "[12/17/2023-07:57:07] [I] Input(s)s format: fp32:CHW\n",
      "[12/17/2023-07:57:07] [I] Output(s)s format: fp32:CHW\n",
      "[12/17/2023-07:57:07] [I] Input build shapes: model\n",
      "[12/17/2023-07:57:07] [I] Input calibration shapes: model\n",
      "[12/17/2023-07:57:07] [I] === System Options ===\n",
      "[12/17/2023-07:57:07] [I] Device: 0\n",
      "[12/17/2023-07:57:07] [I] DLACore: \n",
      "[12/17/2023-07:57:07] [I] Plugins:\n",
      "[12/17/2023-07:57:07] [I] === Inference Options ===\n",
      "[12/17/2023-07:57:07] [I] Batch: Explicit\n",
      "[12/17/2023-07:57:07] [I] Input inference shapes: model\n",
      "[12/17/2023-07:57:07] [I] Iterations: 10\n",
      "[12/17/2023-07:57:07] [I] Duration: 3s (+ 200ms warm up)\n",
      "[12/17/2023-07:57:07] [I] Sleep time: 0ms\n",
      "[12/17/2023-07:57:07] [I] Idle time: 0ms\n",
      "[12/17/2023-07:57:07] [I] Streams: 1\n",
      "[12/17/2023-07:57:07] [I] ExposeDMA: Disabled\n",
      "[12/17/2023-07:57:07] [I] Data transfers: Enabled\n",
      "[12/17/2023-07:57:07] [I] Spin-wait: Disabled\n",
      "[12/17/2023-07:57:07] [I] Multithreading: Disabled\n",
      "[12/17/2023-07:57:07] [I] CUDA Graph: Disabled\n",
      "[12/17/2023-07:57:07] [I] Separate profiling: Disabled\n",
      "[12/17/2023-07:57:07] [I] Time Deserialize: Disabled\n",
      "[12/17/2023-07:57:07] [I] Time Refit: Disabled\n",
      "[12/17/2023-07:57:07] [I] Skip inference: Disabled\n",
      "[12/17/2023-07:57:07] [I] Inputs:\n",
      "[12/17/2023-07:57:07] [I] === Reporting Options ===\n",
      "[12/17/2023-07:57:07] [I] Verbose: Disabled\n",
      "[12/17/2023-07:57:07] [I] Averages: 10 inferences\n",
      "[12/17/2023-07:57:07] [I] Percentile: 99\n",
      "[12/17/2023-07:57:07] [I] Dump refittable layers:Disabled\n",
      "[12/17/2023-07:57:07] [I] Dump output: Disabled\n",
      "[12/17/2023-07:57:07] [I] Profile: Disabled\n",
      "[12/17/2023-07:57:07] [I] Export timing to JSON file: \n",
      "[12/17/2023-07:57:07] [I] Export output to JSON file: \n",
      "[12/17/2023-07:57:07] [I] Export profile to JSON file: \n",
      "[12/17/2023-07:57:07] [I] \n",
      "[12/17/2023-07:57:08] [I] === Device Information ===\n",
      "[12/17/2023-07:57:08] [I] Selected Device: NVIDIA Tegra X1\n",
      "[12/17/2023-07:57:08] [I] Compute Capability: 5.3\n",
      "[12/17/2023-07:57:08] [I] SMs: 1\n",
      "[12/17/2023-07:57:08] [I] Compute Clock Rate: 0.9216 GHz\n",
      "[12/17/2023-07:57:08] [I] Device Global Memory: 3956 MiB\n",
      "[12/17/2023-07:57:08] [I] Shared Memory per SM: 64 KiB\n",
      "[12/17/2023-07:57:08] [I] Memory Bus Width: 64 bits (ECC disabled)\n",
      "[12/17/2023-07:57:08] [I] Memory Clock Rate: 0.01275 GHz\n",
      "[12/17/2023-07:57:08] [I] \n",
      "[12/17/2023-07:57:08] [I] TensorRT version: 8.2.1\n",
      "[12/17/2023-07:57:12] [I] [TRT] [MemUsageChange] Init CUDA: CPU +229, GPU +0, now: CPU 248, GPU 3433 (MiB)\n",
      "[12/17/2023-07:57:12] [I] [TRT] [MemUsageSnapshot] Begin constructing builder kernel library: CPU 248 MiB, GPU 3450 MiB\n",
      "[12/17/2023-07:57:12] [I] [TRT] [MemUsageSnapshot] End constructing builder kernel library: CPU 278 MiB, GPU 3468 MiB\n",
      "[12/17/2023-07:57:12] [I] Start parsing network model\n",
      "[12/17/2023-07:57:13] [I] [TRT] ----------------------------------------------------------------\n",
      "[12/17/2023-07:57:13] [I] [TRT] Input filename:   /nvdli-nano/data/Inference/conv3D_model_best.onnx\n",
      "[12/17/2023-07:57:13] [I] [TRT] ONNX IR version:  0.0.8\n",
      "[12/17/2023-07:57:13] [I] [TRT] Opset version:    17\n",
      "[12/17/2023-07:57:13] [I] [TRT] Producer name:    pytorch\n",
      "[12/17/2023-07:57:13] [I] [TRT] Producer version: 2.1.0\n",
      "[12/17/2023-07:57:13] [I] [TRT] Domain:           \n",
      "[12/17/2023-07:57:13] [I] [TRT] Model version:    0\n",
      "[12/17/2023-07:57:13] [I] [TRT] Doc string:       \n",
      "[12/17/2023-07:57:13] [I] [TRT] ----------------------------------------------------------------\n",
      "[12/17/2023-07:57:13] [I] Finish parsing network model\n",
      "[12/17/2023-07:57:13] [I] [TRT] ---------- Layers Running on DLA ----------\n",
      "[12/17/2023-07:57:13] [I] [TRT] ---------- Layers Running on GPU ----------\n",
      "[12/17/2023-07:57:13] [I] [TRT] [GpuLayer] /conv1/Conv\n",
      "[12/17/2023-07:57:13] [I] [TRT] [GpuLayer] /pool1/MaxPool\n",
      "[12/17/2023-07:57:13] [I] [TRT] [GpuLayer] /conv2/Conv\n",
      "[12/17/2023-07:57:13] [I] [TRT] [GpuLayer] /pool2/MaxPool\n",
      "[12/17/2023-07:57:13] [I] [TRT] [GpuLayer] /conv3/Conv\n",
      "[12/17/2023-07:57:13] [I] [TRT] [GpuLayer] /pool3/MaxPool\n",
      "[12/17/2023-07:57:13] [I] [TRT] [GpuLayer] /flatten/Flatten + (Unnamed Layer* 7) [Shuffle]\n",
      "[12/17/2023-07:57:13] [I] [TRT] [GpuLayer] /fc1/Gemm\n",
      "[12/17/2023-07:57:13] [I] [TRT] [GpuLayer] /fc2/Gemm\n",
      "[12/17/2023-07:57:13] [I] [TRT] [GpuLayer] (Unnamed Layer* 12) [Shuffle]\n",
      "[12/17/2023-07:57:15] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +158, GPU -2, now: CPU 449, GPU 3442 (MiB)\n",
      "[12/17/2023-07:57:23] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +241, GPU +167, now: CPU 690, GPU 3609 (MiB)\n",
      "[12/17/2023-07:57:23] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[12/17/2023-07:57:35] [I] [TRT] Some tactics do not have sufficient workspace memory to run. Increasing workspace size may increase performance, please check verbose output.\n",
      "[12/17/2023-07:57:44] [I] [TRT] Detected 1 inputs and 1 output network tensors.\n",
      "[12/17/2023-07:57:45] [I] [TRT] Total Host Persistent Memory: 800\n",
      "[12/17/2023-07:57:45] [I] [TRT] Total Device Persistent Memory: 0\n",
      "[12/17/2023-07:57:45] [I] [TRT] Total Scratch Memory: 1883648\n",
      "[12/17/2023-07:57:45] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 12 MiB, GPU 29 MiB\n",
      "[12/17/2023-07:57:45] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 0.27651ms to assign 3 blocks to 12 nodes requiring 3849728 bytes.\n",
      "[12/17/2023-07:57:45] [I] [TRT] Total Activation Memory: 3849728\n",
      "[12/17/2023-07:57:45] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 945, GPU 3691 (MiB)\n",
      "[12/17/2023-07:57:45] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +0, now: CPU 945, GPU 3691 (MiB)\n",
      "[12/17/2023-07:57:45] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +16, now: CPU 0, GPU 16 (MiB)\n",
      "[12/17/2023-07:57:45] [I] [TRT] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 945, GPU 3693 (MiB)\n",
      "[12/17/2023-07:57:45] [I] [TRT] Loaded engine size: 13 MiB\n",
      "[12/17/2023-07:57:45] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +2, now: CPU 946, GPU 3695 (MiB)\n",
      "[12/17/2023-07:57:45] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU -3, now: CPU 946, GPU 3691 (MiB)\n",
      "[12/17/2023-07:57:45] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +13, now: CPU 0, GPU 13 (MiB)\n",
      "[12/17/2023-07:57:45] [I] Engine built in 37.1957 sec.\n",
      "[12/17/2023-07:57:45] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 903, GPU 3676 (MiB)\n",
      "[12/17/2023-07:57:45] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +1, GPU +0, now: CPU 904, GPU 3676 (MiB)\n",
      "[12/17/2023-07:57:45] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +3, now: CPU 0, GPU 16 (MiB)\n",
      "[12/17/2023-07:57:45] [I] Using random values for input input.1\n",
      "[12/17/2023-07:57:45] [I] Created input binding for input.1 with dimensions 1x25x3x64x64\n",
      "[12/17/2023-07:57:45] [I] Using random values for output 19\n",
      "[12/17/2023-07:57:45] [I] Created output binding for 19 with dimensions 1x4\n",
      "[12/17/2023-07:57:45] [I] Starting inference\n",
      "[12/17/2023-07:57:48] [I] Warmup completed 7 queries over 200 ms\n",
      "[12/17/2023-07:57:48] [I] Timing trace has 147 queries over 3.06802 s\n",
      "[12/17/2023-07:57:48] [I] \n",
      "[12/17/2023-07:57:48] [I] === Trace details ===\n",
      "[12/17/2023-07:57:48] [I] Trace averages of 10 runs:\n",
      "[12/17/2023-07:57:48] [I] Average on 10 runs - GPU latency: 18.2063 ms - Host latency: 18.3459 ms (end to end 18.3705 ms, enqueue 2.87202 ms)\n",
      "[12/17/2023-07:57:48] [I] Average on 10 runs - GPU latency: 16.2944 ms - Host latency: 16.4188 ms (end to end 16.4325 ms, enqueue 4.04445 ms)\n",
      "[12/17/2023-07:57:48] [I] Average on 10 runs - GPU latency: 16.3146 ms - Host latency: 16.4402 ms (end to end 16.4534 ms, enqueue 4.66304 ms)\n",
      "[12/17/2023-07:57:48] [I] Average on 10 runs - GPU latency: 16.6138 ms - Host latency: 16.739 ms (end to end 16.7523 ms, enqueue 4.4988 ms)\n",
      "[12/17/2023-07:57:48] [I] Average on 10 runs - GPU latency: 20.1818 ms - Host latency: 20.3094 ms (end to end 20.3227 ms, enqueue 5.00292 ms)\n",
      "[12/17/2023-07:57:48] [I] Average on 10 runs - GPU latency: 21.9714 ms - Host latency: 22.1166 ms (end to end 22.1335 ms, enqueue 3.81508 ms)\n",
      "[12/17/2023-07:57:48] [I] Average on 10 runs - GPU latency: 21.2195 ms - Host latency: 21.3701 ms (end to end 21.3832 ms, enqueue 3.03702 ms)\n",
      "[12/17/2023-07:57:48] [I] Average on 10 runs - GPU latency: 18.4666 ms - Host latency: 18.6002 ms (end to end 18.6136 ms, enqueue 4.47969 ms)\n",
      "[12/17/2023-07:57:48] [I] Average on 10 runs - GPU latency: 19.0322 ms - Host latency: 19.1663 ms (end to end 19.1788 ms, enqueue 4.35994 ms)\n",
      "[12/17/2023-07:57:48] [I] Average on 10 runs - GPU latency: 16.225 ms - Host latency: 16.3507 ms (end to end 16.3641 ms, enqueue 3.25081 ms)\n",
      "[12/17/2023-07:57:48] [I] Average on 10 runs - GPU latency: 21.825 ms - Host latency: 21.9532 ms (end to end 22.0578 ms, enqueue 4.08831 ms)\n",
      "[12/17/2023-07:57:48] [I] Average on 10 runs - GPU latency: 22.9052 ms - Host latency: 23.051 ms (end to end 23.09 ms, enqueue 2.91594 ms)\n",
      "[12/17/2023-07:57:48] [I] Average on 10 runs - GPU latency: 26.6305 ms - Host latency: 26.7588 ms (end to end 26.8441 ms, enqueue 3.07209 ms)\n",
      "[12/17/2023-07:57:48] [I] Average on 10 runs - GPU latency: 26.6976 ms - Host latency: 26.8291 ms (end to end 27.3155 ms, enqueue 2.94072 ms)\n",
      "[12/17/2023-07:57:48] [I] \n",
      "[12/17/2023-07:57:48] [I] === Performance summary ===\n",
      "[12/17/2023-07:57:48] [I] Throughput: 47.9136 qps\n",
      "[12/17/2023-07:57:48] [I] Latency: min = 16.0264 ms, max = 46.0383 ms, mean = 20.7539 ms, median = 18.5581 ms, percentile(99%) = 37.927 ms\n",
      "[12/17/2023-07:57:48] [I] End-to-End Host Latency: min = 16.0371 ms, max = 46.0535 ms, mean = 20.8703 ms, median = 18.5702 ms, percentile(99%) = 39.3308 ms\n",
      "[12/17/2023-07:57:48] [I] Enqueue Time: min = 1.85791 ms, max = 7.08453 ms, mean = 3.73828 ms, median = 3.55396 ms, percentile(99%) = 7.01575 ms\n",
      "[12/17/2023-07:57:48] [I] H2D Latency: min = 0.116821 ms, max = 0.221802 ms, mean = 0.129331 ms, median = 0.121826 ms, percentile(99%) = 0.216187 ms\n",
      "[12/17/2023-07:57:48] [I] GPU Compute Time: min = 15.9006 ms, max = 45.8218 ms, mean = 20.6201 ms, median = 18.4011 ms, percentile(99%) = 37.7893 ms\n",
      "[12/17/2023-07:57:48] [I] D2H Latency: min = 0.00219727 ms, max = 0.0136719 ms, mean = 0.00442287 ms, median = 0.00439453 ms, percentile(99%) = 0.00561523 ms\n",
      "[12/17/2023-07:57:48] [I] Total Host Walltime: 3.06802 s\n",
      "[12/17/2023-07:57:48] [I] Total GPU Compute Time: 3.03116 s\n",
      "[12/17/2023-07:57:48] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[12/17/2023-07:57:48] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8201] # /usr/src/tensorrt/bin/trtexec --onnx=/nvdli-nano/data/Inference/conv3D_model_best.onnx --saveEngine=/nvdli-nano/data/Inference/conv3D_model_best.trt\n"
     ]
    }
   ],
   "source": [
    "!/usr/src/tensorrt/bin/trtexec --onnx=/nvdli-nano/data/Inference/conv3D_model_best.onnx --saveEngine=/nvdli-nano/data/Inference/conv3D_model_best.trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnx\n",
      "  Downloading https://files.pythonhosted.org/packages/8f/71/1543d8dad6a26df1da8953653ebdbedacea9f1a5bcd023fe10f8c5f66d63/onnx-1.14.1.tar.gz (11.3MB)\n",
      "\u001b[K    100% |################################| 11.3MB 45kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from onnx)\n",
      "Collecting protobuf>=3.20.2 (from onnx)\n",
      "  Downloading https://files.pythonhosted.org/packages/6c/be/4e32d02bf08b8f76bf6e59f2a531690c1e4264530404501f3489ca975d9a/protobuf-4.21.0-py2.py3-none-any.whl (164kB)\n",
      "\u001b[K    100% |################################| 174kB 2.7MB/s eta 0:00:01\n",
      "\u001b[31mprotobuf requires Python '>=3.7' but the running Python is 3.6.9\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip3 install onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 14173,
     "status": "ok",
     "timestamp": 1700784118638,
     "user": {
      "displayName": "TUSHAR PRASANNA SWAMINATHAN",
      "userId": "06287125988566296846"
     },
     "user_tz": 300
    },
    "id": "3SOhCIt0GHBt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Import the required libraries.\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Specify the device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 93240,
     "status": "ok",
     "timestamp": 1700784211866,
     "user": {
      "displayName": "TUSHAR PRASANNA SWAMINATHAN",
      "userId": "06287125988566296846"
     },
     "user_tz": 300
    },
    "id": "ign5SFqksgDE"
   },
   "outputs": [],
   "source": [
    "# Discard the output of this cell.\n",
    "%%capture\n",
    "\n",
    "# Downlaod the UCF50 Dataset\n",
    "!wget --no-check-certificate https://www.crcv.ucf.edu/data/UCF50.rar\n",
    "\n",
    "#Extract the Dataset\n",
    "!unrar x UCF50.rar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUttlme3vxwR"
   },
   "source": [
    "##   Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1700784211866,
     "user": {
      "displayName": "TUSHAR PRASANNA SWAMINATHAN",
      "userId": "06287125988566296846"
     },
     "user_tz": 300
    },
    "id": "1DnaeJLxdnMr"
   },
   "outputs": [],
   "source": [
    "def frames_extraction(video_path):\n",
    "    '''\n",
    "    This function will extract the required frames from a video after resizing and normalizing them.\n",
    "    Args:\n",
    "        video_path: The path of the video in the disk, whose frames are to be extracted.\n",
    "    Returns:\n",
    "        frames_list: A list containing the resized and normalized frames of the video.\n",
    "    '''\n",
    "\n",
    "    # Declare a list to store video frames.\n",
    "    frames_list = []\n",
    "\n",
    "    # Read the Video File using the VideoCapture object.\n",
    "    video_reader = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Get the total number of frames in the video.\n",
    "    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Calculate the the interval after which frames will be added to the list.\n",
    "    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)\n",
    "\n",
    "    # Iterate through the Video Frames.\n",
    "    for frame_counter in range(SEQUENCE_LENGTH):\n",
    "\n",
    "        # Set the current frame position of the video.\n",
    "        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n",
    "\n",
    "        # Reading the frame from the video.\n",
    "        success, frame = video_reader.read()\n",
    "\n",
    "        # Check if Video frame is not successfully read then break the loop\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        # Resize the Frame to fixed height and width.\n",
    "        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "\n",
    "        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1\n",
    "        normalized_frame = resized_frame / 255\n",
    "\n",
    "        # Append the normalized frame into the frames list\n",
    "        frames_list.append(normalized_frame)\n",
    "\n",
    "    # Release the VideoCapture object.\n",
    "    video_reader.release()\n",
    "\n",
    "    # Return the frames list.\n",
    "    return frames_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1700784211866,
     "user": {
      "displayName": "TUSHAR PRASANNA SWAMINATHAN",
      "userId": "06287125988566296846"
     },
     "user_tz": 300
    },
    "id": "NZh6IJnkdnMr"
   },
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    '''\n",
    "    This function will extract the data of the selected classes and create the required dataset.\n",
    "    Returns:\n",
    "        features:          A list containing the extracted frames of the videos.\n",
    "        labels:            A list containing the indexes of the classes associated with the videos.\n",
    "        video_files_paths: A list containing the paths of the videos in the disk.\n",
    "    '''\n",
    "\n",
    "    # Declared Empty Lists to store the features, labels and video file path values.\n",
    "    features = []\n",
    "    labels = []\n",
    "    video_files_paths = []\n",
    "\n",
    "    # Iterating through all the classes mentioned in the classes list\n",
    "    for class_index, class_name in enumerate(CLASSES_LIST):\n",
    "\n",
    "        # Display the name of the class whose data is being extracted.\n",
    "        print(f'Extracting Data of Class: {class_name}')\n",
    "\n",
    "        # Get the list of video files present in the specific class name directory.\n",
    "        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))\n",
    "\n",
    "        # Iterate through all the files present in the files list.\n",
    "        for file_name in files_list:\n",
    "\n",
    "            # Get the complete video path.\n",
    "            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)\n",
    "\n",
    "            # Extract the frames of the video file.\n",
    "            frames = frames_extraction(video_file_path)\n",
    "\n",
    "            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.\n",
    "            # So ignore the vides having frames less than the SEQUENCE_LENGTH.\n",
    "            if len(frames) == SEQUENCE_LENGTH:\n",
    "\n",
    "                # Append the data to their repective lists.\n",
    "                features.append(frames)\n",
    "                labels.append(class_index)\n",
    "                video_files_paths.append(video_file_path)\n",
    "\n",
    "    # Converting the list to numpy arrays\n",
    "    features = np.asarray(features)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Return the frames, class index, and video file path.\n",
    "    return features, labels, video_files_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 64168,
     "status": "ok",
     "timestamp": 1700784276030,
     "user": {
      "displayName": "TUSHAR PRASANNA SWAMINATHAN",
      "userId": "06287125988566296846"
     },
     "user_tz": 300
    },
    "id": "4_q6tn37tq9I",
    "outputId": "31b7ea02-339b-4b4f-da84-d7f3321ea3ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Data of Class: WalkingWithDog\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'UCF50/WalkingWithDog'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2c55bd48e55c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Create the dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_files_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-0333a5942d34>\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Get the list of video files present in the specific class name directory.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mfiles_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Iterate through all the files present in the files list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'UCF50/WalkingWithDog'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "seed_constant = 27\n",
    "torch.manual_seed(seed_constant)\n",
    "np.random.seed(seed_constant)\n",
    "random.seed(seed_constant)\n",
    "\n",
    "# Specify the height and width to which each video frame will be resized in our dataset.\n",
    "IMAGE_HEIGHT, IMAGE_WIDTH = 64, 64\n",
    "\n",
    "# Specify the number of frames of a video that will be fed to the model as one sequence.\n",
    "SEQUENCE_LENGTH = 25\n",
    "\n",
    "# Specify the directory containing the UCF50 dataset.\n",
    "DATASET_DIR = \"UCF50\"\n",
    "\n",
    "# Specify the list containing the names of the classes used for training. Choose any set of classes.\n",
    "CLASSES_LIST = [\"WalkingWithDog\", \"TaiChi\", \"Swing\", \"HorseRace\"]\n",
    "\n",
    "# Define a PyTorch dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frames = self.features[idx]\n",
    "        frames = [self.transform(frame.astype(np.float32)) for frame in frames]\n",
    "        frames = torch.stack(frames)\n",
    "        return frames, torch.tensor(self.labels[idx])\n",
    "\n",
    "\n",
    "# Create the dataset.\n",
    "features, labels, video_files_paths = create_dataset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1zbq9Aq15Rw7qmkzyiNpnNXfnXssd3Jym"
    },
    "executionInfo": {
     "elapsed": 3180,
     "status": "ok",
     "timestamp": 1700784279198,
     "user": {
      "displayName": "TUSHAR PRASANNA SWAMINATHAN",
      "userId": "06287125988566296846"
     },
     "user_tz": 300
    },
    "id": "qZ2FGMa5uV9I",
    "outputId": "62b36545-0fe6-4610-f156-58000994aae7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Display a random subset of videos\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i in range(1, 21):\n",
    "    random_index = random.randint(0, len(features) - 1)\n",
    "    selected_class_name = CLASSES_LIST[labels[random_index]]\n",
    "    selected_video_path = video_files_paths[random_index]\n",
    "\n",
    "    # Read the first frame of the video file.\n",
    "    video_reader = cv2.VideoCapture(selected_video_path)\n",
    "    _, bgr_frame = video_reader.read()\n",
    "    video_reader.release()\n",
    "\n",
    "    # Convert the frame from BGR into RGB format.\n",
    "    rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Write the class name on the video frame.\n",
    "    cv2.putText(rgb_frame, selected_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "    plt.subplot(5, 4, i)\n",
    "    plt.imshow(rgb_frame)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1700784279199,
     "user": {
      "displayName": "TUSHAR PRASANNA SWAMINATHAN",
      "userId": "06287125988566296846"
     },
     "user_tz": 300
    },
    "id": "vh2_02_NuXvI"
   },
   "outputs": [],
   "source": [
    "# Using PyTorch's DataLoader to handle batching and shuffling\n",
    "dataset = CustomDataset(features, labels)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1700784279199,
     "user": {
      "displayName": "TUSHAR PRASANNA SWAMINATHAN",
      "userId": "06287125988566296846"
     },
     "user_tz": 300
    },
    "id": "ZLrnEfmSuB2T"
   },
   "outputs": [],
   "source": [
    "# Define the 3D CNN model in PyTorch\n",
    "class Conv3DModel(nn.Module):\n",
    "    def __init__(self, num_classes=len(CLASSES_LIST), num_frames=SEQUENCE_LENGTH):\n",
    "        super(Conv3DModel, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(num_frames, 32, kernel_size=(3, 3, 3), padding=1)\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "        self.dropout1 = nn.Dropout3d(0.4)\n",
    "\n",
    "        self.conv2 = nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=1)\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "        self.dropout2 = nn.Dropout3d(0.4)\n",
    "\n",
    "        self.conv3 = nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=1)\n",
    "        self.pool3 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "        self.dropout3 = nn.Dropout3d(0.4)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * (SEQUENCE_LENGTH // 8) * (IMAGE_HEIGHT // 8) * (IMAGE_WIDTH // 8), 128)\n",
    "        self.dropout4 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Create an instance of the model\n",
    "conv3D_model = Conv3DModel()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(conv3D_model.parameters(), lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 346292,
     "status": "ok",
     "timestamp": 1700431845901,
     "user": {
      "displayName": "TUSHAR PRASANNA SWAMINATHAN",
      "userId": "06287125988566296846"
     },
     "user_tz": 300
    },
    "id": "GrZXGIo1ubW_",
    "outputId": "77cee880-2115-4d50-c3f7-5f3cdabab048"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 29.5064, Validation Accuracy: 0.1633\n",
      "Epoch 2/10, Loss: 12.5289, Validation Accuracy: 0.2755\n",
      "Epoch 3/10, Loss: 2.6092, Validation Accuracy: 0.2755\n",
      "Epoch 4/10, Loss: 2.8312, Validation Accuracy: 0.3980\n",
      "Epoch 5/10, Loss: 0.5683, Validation Accuracy: 0.4898\n",
      "Epoch 6/10, Loss: 1.2543, Validation Accuracy: 0.5714\n",
      "Epoch 7/10, Loss: 2.0865, Validation Accuracy: 0.6224\n",
      "Epoch 8/10, Loss: 2.8411, Validation Accuracy: 0.4184\n",
      "Epoch 9/10, Loss: 0.4220, Validation Accuracy: 0.6837\n",
      "Epoch 10/10, Loss: 0.6089, Validation Accuracy: 0.4592\n",
      "Conv3D model saved successfully!\n",
      "Validation Accuracy: 0.45918367346938777\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    conv3D_model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = conv3D_model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validate the model\n",
    "    conv3D_model.eval()\n",
    "    with torch.no_grad():\n",
    "        total, correct = 0, 0\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = conv3D_model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.squeeze()).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Validation Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Save the model\n",
    "torch.save(conv3D_model.state_dict(), 'conv3D_model.pth')\n",
    "print(\"Conv3D model saved successfully!\")\n",
    "\n",
    "# Evaluate the trained model.\n",
    "conv3D_model.eval()\n",
    "total, correct = 0, 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        outputs = conv3D_model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.squeeze()).sum().item()\n",
    "\n",
    "model_evaluation_accuracy = correct / total\n",
    "print(\"Validation Accuracy:\", model_evaluation_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13317,
     "status": "ok",
     "timestamp": 1700784536892,
     "user": {
      "displayName": "TUSHAR PRASANNA SWAMINATHAN",
      "userId": "06287125988566296846"
     },
     "user_tz": 300
    },
    "id": "keQvfhXX03VX",
    "outputId": "349bb5bc-0793-4d65-d793-1620a13e1888"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnx\n",
      "  Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx) (1.23.5)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
      "Installing collected packages: onnx\n",
      "Successfully installed onnx-1.15.0\n",
      "Model successfully exported to ONNX format.\n"
     ]
    }
   ],
   "source": [
    "import torch.onnx\n",
    "import torch.nn as nn\n",
    "!pip install onnx\n",
    "\n",
    "# Create an instance of the model\n",
    "conv3D_model = Conv3DModel()\n",
    "\n",
    "# Load the trained weights\n",
    "conv3D_model.load_state_dict(torch.load('/content/drive/MyDrive/Project Phase 1/3DCNN/conv3D_model.pth'))\n",
    "\n",
    "# Set the model to evaluation mode before exporting\n",
    "conv3D_model.eval()\n",
    "\n",
    "# Create a dummy input tensor to trace the model\n",
    "dummy_input = torch.randn(1, SEQUENCE_LENGTH, 3, IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "\n",
    "# Provide the path where you want to save the ONNX model\n",
    "onnx_path = '/content/drive/MyDrive/Project Phase 1/3DCNN/conv3D_model.onnx'\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(conv3D_model, dummy_input, onnx_path, verbose=True)\n",
    "\n",
    "print(\"Model successfully exported to ONNX format.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
